{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /home/guest/anaconda3/lib/python3.12/site-packages (4.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/guest/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in /home/guest/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/guest/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in /home/guest/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in /home/guest/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/guest/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (0.31.2)\n",
      "Requirement already satisfied: Pillow in /home/guest/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/guest/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in /home/guest/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/guest/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/guest/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/guest/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/guest/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/guest/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/guest/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/guest/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/guest/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /home/guest/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/guest/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/guest/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/guest/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/guest/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/guest/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/guest/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/guest/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/guest/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/guest/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/guest/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/guest/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/guest/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/guest/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/guest/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/guest/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/guest/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/guest/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/guest/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/guest/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/guest/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/guest/anaconda3/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/guest/anaconda3/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/guest/anaconda3/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/guest/anaconda3/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.8.30)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/guest/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/guest/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import enum\n",
    "import timeit\n",
    "\n",
    "# Third-Party Libraries\n",
    "import pymupdf  # for PDF parsing  #type: ignore[import]\n",
    "from docx import Document #type: ignore[import]\n",
    "from pptx import Presentation #type: ignore[import]\n",
    "from nltk import download #type: ignore[import]\n",
    "from nltk.corpus import stopwords #type: ignore[import]\n",
    "from sentence_transformers import SentenceTransformer #type: ignore[import]\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql import SparkSession #type: ignore[import]\n",
    "from pyspark.ml.feature import Word2Vec #type: ignore[import]\n",
    "from pyspark.sql.types import StructType, StructField #type: ignore[import]\n",
    "import pyspark.sql.types as T #type: ignore[import]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 : Preprocessing, Tokenization, and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to set the following environment variable, so that Spark knows where YARN runs\n",
    "os.environ['HADOOP_CONF_DIR']=\"/etc/hadoop/conf\"\n",
    "\n",
    "# Since we are accessing spark through it's python API, we need to make sure that all executor\n",
    "# instances run the same version of python. \n",
    "# (and we want Anaconda to be used, so we have access to numpy, pandas, and so forth)\n",
    "# You will likely need to adjust this path if your run on a different cluster\n",
    "os.environ['PYSPARK_PYTHON']=\"/home/guest/anaconda3/bin/python\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']=\"/home/guest/anaconda3/bin/python\"\n",
    "\n",
    "\n",
    "#The following lines are just there to allow this cell to be re-executed multiple times:\n",
    "#if a spark session was already started, we stop it before starting a new one\n",
    "#(there can be only one spark context per jupyter notebook)\n",
    "if 'spark' in locals():\n",
    "    spark.stop() #type: ignore[no-untyped-call]\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"tokenizer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#When dealing with RDDs, we work the sparkContext object. See https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we have a working spark context, print its configuration\n",
    "sc._conf.getAll()\n",
    "class EmbeddingStrategy(enum.Enum):\n",
    "    WORD2VEC = \"word2vec\"\n",
    "    SENTENCE_TRANSFORMERS = \"sentence-transformers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting documents to raw text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/guest/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved in /dataset_txt\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "PARALLELISM = 8  # number of partitions\n",
    "CHUNK_SIZE = 300  # must be between 200 and 500\n",
    "EMBEDDING_STRATEGY = EmbeddingStrategy.SENTENCE_TRANSFORMERS  # or SENTENCE_TRANSFORMERS\n",
    "INPUT_DIR = \"../dataset\"\n",
    "TXT_DIR = \"../dataset_txt\"\n",
    "OUTPUT_DIR = \"../dataset/parquet\"\n",
    "os.makedirs(TXT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# Cleaning\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'^[\\s]*[•*·\\-–—]+[\\s]*', '', text, flags=re.MULTILINE)  # bullet chars at line starts\n",
    "    text = re.sub(r'http\\S+|www\\S+|@\\w+|#\\w+', '', text)     # URLs, hashtags, mentions\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text, flags=re.UNICODE)  # punctuation\n",
    "    text = text.lower()                                     # lowercase\n",
    "    tokens = text.split()                                   # tokenize\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 1]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Loop through files in the dataset directory\n",
    "for filename in os.listdir(INPUT_DIR):\n",
    "    filepath = os.path.join(INPUT_DIR, filename)\n",
    "    text = \"\"\n",
    "\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        try:\n",
    "            with pymupdf.open(filepath) as pdf:\n",
    "                for page in pdf:\n",
    "                    text += page.get_text()\n",
    "        except Exception as e:\n",
    "            print(f\"Error while reading PDF : {e}\")\n",
    "            continue\n",
    "\n",
    "    elif filename.endswith(\".docx\"):\n",
    "        try:\n",
    "            doc = Document(filepath)\n",
    "            text = \"\\n\".join(p.text for p in doc.paragraphs)\n",
    "        except Exception as e:\n",
    "            print(f\"Error while reading DOCX : {e}\")\n",
    "            continue\n",
    "\n",
    "    elif filename.endswith(\".pptx\"):\n",
    "        try:\n",
    "            prs = Presentation(filepath)\n",
    "            parts = []\n",
    "            for slide in prs.slides:\n",
    "                for shape in slide.shapes:\n",
    "                    if hasattr(shape, \"text\"):\n",
    "                        parts.append(shape.text)\n",
    "            text = \"\\n\".join(parts)\n",
    "        except Exception as e:\n",
    "            print(f\"Error while reading PPTX : {e}\")\n",
    "            continue\n",
    "        \n",
    "    # Clean and save the text\n",
    "    cleaned_text = clean_text(text)\n",
    "    output_path = os.path.join(TXT_DIR, filename.rsplit(\".\", 1)[0] + \".txt\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(cleaned_text)\n",
    "        \n",
    "print(\"Files saved in /dataset_txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# use spark to read all text files in the directory ../dataset_txt\n",
    "RDD = sc.textFile(f\"{TXT_DIR}/*.txt\")\n",
    "RDD = RDD.repartition(PARALLELISM)\n",
    "print(RDD.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  ['info',\n",
       "   'h515',\n",
       "   'big',\n",
       "   'data',\n",
       "   'distributed',\n",
       "   'management',\n",
       "   'lecture',\n",
       "   'distributed',\n",
       "   'processing',\n",
       "   'spark',\n",
       "   'dimitris',\n",
       "   'sacharidis',\n",
       "   'outline',\n",
       "   'introduction',\n",
       "   'resilient',\n",
       "   'distributed',\n",
       "   'datasets',\n",
       "   'rdds',\n",
       "   'operations',\n",
       "   'rdds',\n",
       "   'creation',\n",
       "   'transformations',\n",
       "   'actions',\n",
       "   'special',\n",
       "   'global',\n",
       "   'variables',\n",
       "   'pair',\n",
       "   'rdds',\n",
       "   'spark',\n",
       "   'program',\n",
       "   'execution',\n",
       "   'lazy',\n",
       "   'evaluation',\n",
       "   'caching',\n",
       "   'persistence',\n",
       "   'master',\n",
       "   'slave',\n",
       "   'architecture',\n",
       "   'running',\n",
       "   'spark',\n",
       "   'cluster',\n",
       "   'apache',\n",
       "   'spark',\n",
       "   'eco',\n",
       "   'system',\n",
       "   'reading',\n",
       "   '96',\n",
       "   'outline',\n",
       "   'introduction',\n",
       "   'resilient',\n",
       "   'distributed',\n",
       "   'datasets',\n",
       "   'rdds',\n",
       "   'operations',\n",
       "   'rdds',\n",
       "   'creation',\n",
       "   'transformations',\n",
       "   'actions',\n",
       "   'special',\n",
       "   'global',\n",
       "   'variables',\n",
       "   'pair',\n",
       "   'rdds',\n",
       "   'spark',\n",
       "   'program',\n",
       "   'execution',\n",
       "   'lazy',\n",
       "   'evaluation',\n",
       "   'caching',\n",
       "   'persistence',\n",
       "   'master',\n",
       "   'slave',\n",
       "   'architecture',\n",
       "   'running',\n",
       "   'spark',\n",
       "   'cluster',\n",
       "   'apache',\n",
       "   'spark',\n",
       "   'eco',\n",
       "   'system',\n",
       "   'reading',\n",
       "   '96',\n",
       "   'spark',\n",
       "   '96',\n",
       "   'apache',\n",
       "   'spark',\n",
       "   'fast',\n",
       "   'memory',\n",
       "   'distributed',\n",
       "   'data',\n",
       "   'processing',\n",
       "   'engine',\n",
       "   'spark',\n",
       "   '96',\n",
       "   'apache',\n",
       "   'spark',\n",
       "   'fast',\n",
       "   'memory',\n",
       "   'distributed',\n",
       "   'data',\n",
       "   'processing',\n",
       "   'engine',\n",
       "   'speed',\n",
       "   'run',\n",
       "   'computations',\n",
       "   'memory',\n",
       "   '100',\n",
       "   'times',\n",
       "   'faster',\n",
       "   'mapreduce',\n",
       "   'running',\n",
       "   'fully',\n",
       "   'memory',\n",
       "   '10',\n",
       "   'times',\n",
       "   'faster',\n",
       "   'mapreduce',\n",
       "   'even',\n",
       "   'running',\n",
       "   'disk',\n",
       "   'spark',\n",
       "   '96',\n",
       "   'apache',\n",
       "   'spark',\n",
       "   'fast',\n",
       "   'memory',\n",
       "   'distributed',\n",
       "   'data',\n",
       "   'processing',\n",
       "   'engine',\n",
       "   'supports',\n",
       "   'efficient',\n",
       "   'execution',\n",
       "   'different',\n",
       "   'kinds',\n",
       "   'workloads',\n",
       "   'batch',\n",
       "   'streaming',\n",
       "   'machine',\n",
       "   'learning',\n",
       "   'sql',\n",
       "   'ease',\n",
       "   'programming',\n",
       "   'general',\n",
       "   'programming',\n",
       "   'model',\n",
       "   'based',\n",
       "   'composing',\n",
       "   'arbitrary',\n",
       "   'operators',\n",
       "   'allows',\n",
       "   'combining',\n",
       "   'different',\n",
       "   'processing',\n",
       "   'models',\n",
       "   'seamlessly',\n",
       "   'application',\n",
       "   'data',\n",
       "   'classification',\n",
       "   'spark',\n",
       "   'machine',\n",
       "   'learning',\n",
       "   'library',\n",
       "   'streaming',\n",
       "   'data',\n",
       "   'source',\n",
       "   'via',\n",
       "   'spark',\n",
       "   'streaming',\n",
       "   'querying',\n",
       "   'resulting',\n",
       "   'data',\n",
       "   'real',\n",
       "   'time',\n",
       "   'spark',\n",
       "   'sql',\n",
       "   'brief',\n",
       "   'history',\n",
       "   'spark',\n",
       "   '2009',\n",
       "   'spark',\n",
       "   'created',\n",
       "   'uc',\n",
       "   'berkeley',\n",
       "   'lab',\n",
       "   'later',\n",
       "   'becomes',\n",
       "   'amplab',\n",
       "   'created',\n",
       "   'matei',\n",
       "   'zaharia',\n",
       "   'phd',\n",
       "   'studies',\n",
       "   'ion',\n",
       "   'stoica',\n",
       "   'also',\n",
       "   'designed',\n",
       "   'core',\n",
       "   'scheduling',\n",
       "   'algorithms',\n",
       "   'used',\n",
       "   'apache',\n",
       "   'hadoop',\n",
       "   '2010',\n",
       "   'open',\n",
       "   'sourced',\n",
       "   'bsd',\n",
       "   'license',\n",
       "   '2013',\n",
       "   'spark',\n",
       "   'donated',\n",
       "   'apache',\n",
       "   'software',\n",
       "   'foundation',\n",
       "   'also',\n",
       "   'founding',\n",
       "   'databricks',\n",
       "   'original',\n",
       "   'creators',\n",
       "   'commercialize',\n",
       "   'spark',\n",
       "   '2014',\n",
       "   'becomes',\n",
       "   'top',\n",
       "   'level',\n",
       "   'apache',\n",
       "   'project',\n",
       "   '2016',\n",
       "   'spark',\n",
       "   'released',\n",
       "   'sql2003',\n",
       "   'support',\n",
       "   '2020',\n",
       "   'spark',\n",
       "   'released',\n",
       "   'matei',\n",
       "   'zaharia',\n",
       "   'ion',\n",
       "   'stoica',\n",
       "   '96',\n",
       "   'outline',\n",
       "   'introduction',\n",
       "   'resilient',\n",
       "   'distributed',\n",
       "   'datasets',\n",
       "   'rdds',\n",
       "   'operations',\n",
       "   'rdds',\n",
       "   'creation',\n",
       "   'transformations',\n",
       "   'actions',\n",
       "   'special',\n",
       "   'global',\n",
       "   'variables',\n",
       "   'pair',\n",
       "   'rdds',\n",
       "   'spark',\n",
       "   'program',\n",
       "   'execution',\n",
       "   'lazy',\n",
       "   'evaluation',\n",
       "   'caching',\n",
       "   'persistence',\n",
       "   'master',\n",
       "   'slave',\n",
       "   'architecture',\n",
       "   'running',\n",
       "   'spark',\n",
       "   'cluster',\n",
       "   'apache',\n",
       "   'spark',\n",
       "   'eco',\n",
       "   'system',\n",
       "   'reading',\n",
       "   '96',\n",
       "   'rdd',\n",
       "   'rdd',\n",
       "   'resilient',\n",
       "   'distributed',\n",
       "   'dataset',\n",
       "   'spark',\n",
       "   'abstraction',\n",
       "   'representing',\n",
       "   'large',\n",
       "   'dataset',\n",
       "   'rdd',\n",
       "   'collection',\n",
       "   'elements',\n",
       "   'rdds',\n",
       "   'contain',\n",
       "   'types',\n",
       "   'objects',\n",
       "   'elements',\n",
       "   'including',\n",
       "   'user',\n",
       "   'defined',\n",
       "   'classes',\n",
       "   'hood',\n",
       "   'spark',\n",
       "   'automatically']),\n",
       " (8,\n",
       "  ['think',\n",
       "   'rdd',\n",
       "   'consisting',\n",
       "   'instructions',\n",
       "   'compute',\n",
       "   'data',\n",
       "   'build',\n",
       "   'transformations',\n",
       "   'spark',\n",
       "   'uses',\n",
       "   'lazy',\n",
       "   'evaluation',\n",
       "   'reduce',\n",
       "   'number',\n",
       "   'passes',\n",
       "   'take',\n",
       "   'data',\n",
       "   'grouping',\n",
       "   'operations',\n",
       "   'together',\n",
       "   '65',\n",
       "   '96',\n",
       "   'rdd',\n",
       "   'lineage',\n",
       "   'graph',\n",
       "   'r00',\n",
       "   'sc',\n",
       "   'parallelize',\n",
       "   'range',\n",
       "   '10',\n",
       "   'r01',\n",
       "   'sc',\n",
       "   'parallelize',\n",
       "   'range',\n",
       "   '100',\n",
       "   '10',\n",
       "   'r10',\n",
       "   'r00',\n",
       "   'cartesian',\n",
       "   'r00',\n",
       "   'r11',\n",
       "   'r00',\n",
       "   'map',\n",
       "   'lambda',\n",
       "   'r12',\n",
       "   'r00',\n",
       "   'zip',\n",
       "   'r01',\n",
       "   'r13',\n",
       "   'r01',\n",
       "   'keyby',\n",
       "   'lambda',\n",
       "   '20',\n",
       "   'r20',\n",
       "   'sc',\n",
       "   'union',\n",
       "   'r10',\n",
       "   'r11',\n",
       "   'r12',\n",
       "   'r13',\n",
       "   'r20',\n",
       "   'r10',\n",
       "   'r11',\n",
       "   'r12',\n",
       "   'r13',\n",
       "   'r00',\n",
       "   'r01',\n",
       "   '66',\n",
       "   '96',\n",
       "   'wide',\n",
       "   'transformations',\n",
       "   'transformations',\n",
       "   'require',\n",
       "   'shuffle',\n",
       "   'phase',\n",
       "   'intersection',\n",
       "   'distinct',\n",
       "   'reducebykey',\n",
       "   'groupbykey',\n",
       "   'join',\n",
       "   'cartesian',\n",
       "   'repartition',\n",
       "   'coalesce',\n",
       "   '67',\n",
       "   '96',\n",
       "   'narrow',\n",
       "   'transformations',\n",
       "   'transformations',\n",
       "   'need',\n",
       "   'shuffle',\n",
       "   'phase',\n",
       "   'grouped',\n",
       "   'spark',\n",
       "   'single',\n",
       "   'stage',\n",
       "   'called',\n",
       "   'pipelining',\n",
       "   'map',\n",
       "   'flatmap',\n",
       "   'mappartition',\n",
       "   'filter',\n",
       "   'sample',\n",
       "   'union',\n",
       "   '68',\n",
       "   '96',\n",
       "   'outline',\n",
       "   'introduction',\n",
       "   'resilient',\n",
       "   'distributed',\n",
       "   'datasets',\n",
       "   'rdds',\n",
       "   'operations',\n",
       "   'rdds',\n",
       "   'creation',\n",
       "   'transformations',\n",
       "   'actions',\n",
       "   'special',\n",
       "   'global',\n",
       "   'variables',\n",
       "   'pair',\n",
       "   'rdds',\n",
       "   'spark',\n",
       "   'program',\n",
       "   'execution',\n",
       "   'lazy',\n",
       "   'evaluation',\n",
       "   'caching',\n",
       "   'persistence',\n",
       "   'master',\n",
       "   'slave',\n",
       "   'architecture',\n",
       "   'running',\n",
       "   'spark',\n",
       "   'cluster',\n",
       "   'apache',\n",
       "   'spark',\n",
       "   'eco',\n",
       "   'system',\n",
       "   'reading',\n",
       "   '69',\n",
       "   '96',\n",
       "   'persistence',\n",
       "   'sometimes',\n",
       "   'would',\n",
       "   'like',\n",
       "   'call',\n",
       "   'actions',\n",
       "   'rdd',\n",
       "   'multiple',\n",
       "   'times',\n",
       "   'naively',\n",
       "   'rdds',\n",
       "   'dependencies',\n",
       "   'recomputed',\n",
       "   'time',\n",
       "   'action',\n",
       "   'called',\n",
       "   'rdd',\n",
       "   'expensive',\n",
       "   'especially',\n",
       "   'iterative',\n",
       "   'algorithms',\n",
       "   'would',\n",
       "   'call',\n",
       "   'actions',\n",
       "   'dataset',\n",
       "   'many',\n",
       "   'times',\n",
       "   'want',\n",
       "   'reuse',\n",
       "   'rdd',\n",
       "   'multiple',\n",
       "   'actions',\n",
       "   'also',\n",
       "   'ask',\n",
       "   'spark',\n",
       "   'persist',\n",
       "   'calling',\n",
       "   'persist',\n",
       "   'method',\n",
       "   'rdd',\n",
       "   'persist',\n",
       "   'rdd',\n",
       "   'first',\n",
       "   'time',\n",
       "   'computed',\n",
       "   'action',\n",
       "   'kept',\n",
       "   'memory',\n",
       "   'across',\n",
       "   'nodes',\n",
       "   'rdd',\n",
       "   'sc',\n",
       "   'parallelize',\n",
       "   'rdd',\n",
       "   'persist',\n",
       "   'is_cached',\n",
       "   'true',\n",
       "   '70',\n",
       "   '96',\n",
       "   'different',\n",
       "   'storage',\n",
       "   'level',\n",
       "   'storage',\n",
       "   'level',\n",
       "   'meaning',\n",
       "   'memory_only',\n",
       "   'store',\n",
       "   'rdd',\n",
       "   'deserialized',\n",
       "   'java',\n",
       "   'objects',\n",
       "   'jvm',\n",
       "   'rdd',\n",
       "   'fit',\n",
       "   'memory',\n",
       "   'partitions',\n",
       "   'cached',\n",
       "   'recomputed',\n",
       "   'fly',\n",
       "   'time',\n",
       "   'needed',\n",
       "   'default',\n",
       "   'level',\n",
       "   'memory_and_disk',\n",
       "   'store',\n",
       "   'rdd',\n",
       "   'deserialized',\n",
       "   'java',\n",
       "   'objects',\n",
       "   'jvm',\n",
       "   'rdd',\n",
       "   'fit',\n",
       "   'memory',\n",
       "   'store',\n",
       "   'partitions',\n",
       "   'fit',\n",
       "   'disk',\n",
       "   'read',\n",
       "   'needed',\n",
       "   'memory_only_ser',\n",
       "   'java',\n",
       "   'scala',\n",
       "   'store',\n",
       "   'rdd',\n",
       "   'serialized',\n",
       "   'java',\n",
       "   'objects',\n",
       "   'one',\n",
       "   'byte',\n",
       "   'array',\n",
       "   'per',\n",
       "   'partition',\n",
       "   'generally',\n",
       "   'space',\n",
       "   'efficient',\n",
       "   'deserialized',\n",
       "   'objects',\n",
       "   'especially',\n",
       "   'using',\n",
       "   'fast',\n",
       "   'serializer',\n",
       "   'cpu',\n",
       "   'intensive',\n",
       "   'read',\n",
       "   'memory_and_disk_ser',\n",
       "   'java',\n",
       "   'scala',\n",
       "   'similar',\n",
       "   'memory_only_ser',\n",
       "   'spill',\n",
       "   'partitions',\n",
       "   'fit',\n",
       "   'memory',\n",
       "   'disk',\n",
       "   'instead',\n",
       "   'recomputing',\n",
       "   'fly',\n",
       "   'time',\n",
       "   'needed',\n",
       "   'dlsk_only',\n",
       "   'store',\n",
       "   'rdd',\n",
       "   'partitions',\n",
       "   'disk',\n",
       "   '71',\n",
       "   '96',\n",
       "   'storage',\n",
       "   'level',\n",
       "   'choose',\n",
       "   'spark',\n",
       "   'storage',\n",
       "   'levels',\n",
       "   'meant',\n",
       "   'provide',\n",
       "   'different',\n",
       "   'trade',\n",
       "   'offs',\n",
       "   'memory',\n",
       "   'usage'])]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapping Step\n",
    "words_RDD = RDD.flatMap(lambda line: line.split()) # Each word becomes an element in the RDD\n",
    "\n",
    "words_RDD = words_RDD.map(lambda w: w.lower()) # map to lower case\n",
    "\n",
    "indexed_words = words_RDD.zipWithIndex()  # (word, id) pairs\n",
    "\n",
    "if CHUNK_SIZE < 200 or CHUNK_SIZE > 500:\n",
    "    raise ValueError(\"chunk_size must be between 200 and 500\")\n",
    "\n",
    "indexed_words.take(10)  # show the first 10 elements\n",
    "\n",
    "# reduce the elements into chunks of size chunk_size\n",
    "pairs_RDD = indexed_words.map(lambda x: (x[1] // CHUNK_SIZE, x[0]))  # (chunk_id, word)\n",
    "pairs_RDD = pairs_RDD.groupByKey().map(lambda x: (x[0], list(x[1])))  # (chunk_id, [word1, word2, ...])\n",
    "pairs_RDD.take(2)  # show the first 10 elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec & SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../dataset/models/\"):\n",
    "    os.makedirs(\"../dataset/models/\")\n",
    "else:\n",
    "    shutil.rmtree(\"../dataset/models/\")\n",
    "    os.makedirs(\"../dataset/models/\")\n",
    "\n",
    "schema = StructType([\n",
    "            StructField(\"id\", T.StringType(), False),\n",
    "            StructField(\"text_chunks\", T.ArrayType(T.StringType()), False),\n",
    "            StructField(\"embedding\", T.ArrayType(T.FloatType()), False)\n",
    "        ])\n",
    "\n",
    "df = pairs_RDD.toDF([\"id\", \"text_chunks\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 1.7099452310067136 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "begin = timeit.default_timer()\n",
    "match EMBEDDING_STRATEGY:\n",
    "    case EmbeddingStrategy.WORD2VEC:\n",
    "        # Use Word2Vec\n",
    "        word2vec = Word2Vec(inputCol=\"text_chunks\", outputCol=\"embedding\")\n",
    "        model = word2vec.fit(df)\n",
    "        result_word2vec = model.transform(df)\n",
    "        #\n",
    "        # Export the file to Parquet\n",
    "        result_word2vec.write.save(f\"{OUTPUT_DIR}/word2vec\",mode=\"overwrite\" ,format=\"parquet\")\n",
    "        model.save(\"../dataset/models/word2vec\")\n",
    "\n",
    "    case EmbeddingStrategy.SENTENCE_TRANSFORMERS:\n",
    "        SBERT = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        def encode_partition(iterator):\n",
    "            model = SBERT\n",
    "            batch = list(iterator)\n",
    "            texts = [\" \".join(row[1]) for row in batch]  # join word tokens into sentences\n",
    "            embeddings = model.encode(texts)\n",
    "            return ((row[0], row[1], emb.tolist()) for row, emb in zip(batch, embeddings))   \n",
    "\n",
    "        #print(\"number of partition\", df.rdd.getNumPartitions())\n",
    "        embeddings_rdd = df.rdd.mapPartitions(encode_partition)\n",
    "        # Convert the RDD to a DataFrame using the schema\n",
    "        result_sbert = spark.createDataFrame(embeddings_rdd, schema)\n",
    "        stop = timeit.default_timer()\n",
    "        result_sbert.write.save(f\"{OUTPUT_DIR}/SBERT\",mode=\"overwrite\" ,format=\"parquet\")\n",
    "    case _:\n",
    "        raise ValueError(\"Invalid embedding strategy. Choose either WORD2VEC or SENTENCE_TRANSFORMERS.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
